# NeurIPS 2023

| **Title** | **Paper** | **Code or Demo** |
|:---------:|:---------:|:--------:|
| LLM-Pruner: On the Structural Pruning of Large Language Models | [![arXiv](https://img.shields.io/badge/arXiv-2305.11627-b31b1b.svg)](https://arxiv.org/abs/2305.11627) | [![GitHub](https://img.shields.io/github/stars/horseee/LLM-Pruner?style=social)](https://github.com/horseee/LLM-Pruner)|
| RADAR: Robust AI-Text Detection via Adversarial Learning | [![arXiv](https://img.shields.io/badge/arXiv-2307.03838-b31b1b.svg)](https://arxiv.org/abs/2307.03838) | [(Demo)](https://radar-app.vizhub.ai/)|
| Causal-structure Driven Augmentations for Text OOD Generalization | [![arXiv](https://img.shields.io/badge/arXiv-2310.12803-b31b1b.svg)](https://arxiv.org/abs/2310.12803) |
| Enhancing Large Language Models with Ensemble of
Critics for Mitigating Toxicity and Hallucination | [(OpenReview)](https://openreview.net/pdf?id=4uiOPSvbN6)|
| Do Language Models Know When They're Hallucinating References? | [![arXiv](https://img.shields.io/badge/arXiv-2305.18248-b31b1b.svg)](https://arxiv.org/abs/2305.18248) |
| "Diversity and Synthetic data" workshop talk by  Adji Bousson Dieng | | [![GitHub](https://img.shields.io/github/stars/horseee/LLM-Pruner?style=social)](https://github.com/vertaix/Vendi-Score) |
| When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations| [![arXiv](https://img.shields.io/badge/arXiv-2310.19698-b31b1b.svg)](https://arxiv.org/abs/2310.19698) |[![GitHub](https://img.shields.io/github/stars/horseee/LLM-Pruner?style=social)](https://github.com/AleksandarPetrov/prefix-tuning-theory) |
| D4: Improving LLM Pretraining via Document
De-Duplication and Diversification | [![arXiv](https://img.shields.io/badge/arXiv-2308.12284-b31b1b.svg)](https://arxiv.org/abs/2308.12284) |
